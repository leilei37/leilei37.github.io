<!DOCTYPE html>
<html>

<head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1, user-scalable=no">

    
    <title>The Summary of Machine learning (StatQuest) | leilei&#39;s blog</title>

    <meta name="description" content="leilei&#39;s blog">
    <meta name="keywords" content="">

    

    <meta property="og:locale" content="cn" />
    <meta property="og:type" content="article" />
    <meta property="og:title" content= "The Summary of Machine learning (StatQuest) | leilei&#39;s blog"  />
    <meta property="og:description" content= "leilei&#39;s blog" />
    <meta property="og:url" content="http://example.com/2023/08/31/ml/index.html" />
    <meta property="og:site_name" content="" />
    <meta property="article:author" content="leilei" />
    <meta property="article:publisher" content="" />
    <meta property="og:description" content="leilei&#39;s blog" />
    <meta name="twitter:title" content="The Summary of Machine learning (StatQuest) | leilei&#39;s blog"/>
    <meta name="twitter:description" content="leilei&#39;s blog"/>
    <script type="application/ld+json">
        {
            "description": "leilei&#39;s blog",
            "author": { "@type": "Person", "name": "leilei" },
            "@type": "BlogPosting",
            "url": "http://example.com/2023/08/31/ml/index.html",
            "publisher": {
            "@type": "Organization",
            "logo": {
                "@type": "ImageObject",
                "url": "http://example.comundefined"
            },
            "name": "leilei"
            },
            "headline": "The Summary of Machine learning (StatQuest) | leilei&#39;s blog",
            "datePublished": "2023-08-31T00:45:37.000Z",
            "mainEntityOfPage": {
                "@type": "WebPage",
                "@id": "http://example.com/2023/08/31/ml/index.html"
            },
            "@context": "http://schema.org"
        }
    </script>




    

    

    

    

    

    

    

    
<link rel="stylesheet" href="/dist/build.css?v=1654266144177.css">


    
<link rel="stylesheet" href="/dist/custom.css?v=1654266144177.css">


    <script>
        window.isPost = true
        window.aomori = {
            
            
            
        }
        window.aomori_logo_typed_animated = false
        window.aomori_search_algolia = false

    </script>

<meta name="generator" content="Hexo 6.3.0"></head>

<body>

    <div class="container">
    <header class="header">
        <div class="header-type">
            
            <div class="header-type-inner">
                
                    <a class="header-type-title" href="/">leilei&#39;s blog</a>
                
    
                
            </div>
        </div>
        <div class="header-menu">
            <div class="header-menu-inner">
                
                <a href="/">Home</a>
                
                <a href="/archives">Archives</a>
                
                <a href="/about">About</a>
                
            </div>
            <div class="header-menu-social">
                
    <a class="social" target="_blank" href="mailto:abigailupup@gmail.com">
        <ion-icon name="mail-outline"></ion-icon>
    </a>

    <a class="social" target="_blank" href="">
        <ion-icon name=""></ion-icon>
    </a>

            </div>
        </div>

        <div class="header-menu-mobile">
            <div class="header-menu-mobile-inner" id="mobile-menu-open">
                <i class="icon icon-menu"></i>
            </div>
        </div>
    </header>

    <div class="header-menu-mobile-menu">
        <div class="header-menu-mobile-menu-bg"></div>
        <div class="header-menu-mobile-menu-wrap">
            <div class="header-menu-mobile-menu-inner">
                <div class="header-menu-mobile-menu-close" id="mobile-menu-close">
                    <i class="icon icon-cross"></i>
                </div>
                <div class="header-menu-mobile-menu-list">
                    
                    <a href="/">Home</a>
                    
                    <a href="/archives">Archives</a>
                    
                    <a href="/about">About</a>
                    
                </div>
            </div>
        </div>
    </div>

</div>

    <div class="container">
        <div class="main">
            <section class="inner">
                <section class="inner-main">
                    <div class="post">
    <article id="post-clm2gqwv50000m7w87sryfoy8" class="article article-type-post" itemscope
    itemprop="blogPost">

    <div class="article-inner">

        
          
        
        
        

        
        <header class="article-header">
            
  
    <h1 class="article-title" itemprop="name">
      The Summary of Machine learning (StatQuest)
    </h1>
  

        </header>
        

        <div class="article-more-info article-more-info-post hairline">

            <div class="article-date">
  <time datetime="2023-08-31T00:45:37.000Z" itemprop="datePublished">2023-08-31</time>
</div>

            

            

            

        </div>

        <div class="article-entry post-inner-html hairline" itemprop="articleBody">
            <p>cross validation<br>sensitivity：the TP(true positive) were correctly identified by the model. TP&#x2F;(TP+FN)<br>specificity:the TN(true negative) were correctly identified by the model. TN&#x2F;(TN+FP)<br>bias &amp; variance : overfit<br>3 commonly model: regularization, boosting, and bagging(random forest)<br>ROC (FPR,TPR): to choose the optimal threshold. higher y, lower x.<br>AUC (ROC下面积): to choose the model. ROC与x轴围成的面积，越大越好<br>precision&#x3D;TP&#x2F;(TP+FP) [研究罕见病时使用，因为此时TN多</p>
<h2 id="calculate-AUC"><a href="#calculate-AUC" class="headerlink" title="calculate AUC"></a>calculate AUC</h2><p><a target="_blank" rel="noopener" href="https://github.com/StatQuest/roc_and_auc_demo/blob/master/roc_and_auc_demo.R">code from StatQuest</a><br><code>par(pty = &quot;s&quot;)</code> #make the plot as a square</p>
<p>Entropy:value of surprise<br>mutual information; joint probility; marginal probility<br><img src="/2023/08/31/ml/ml.jpg"> </p>
<h2 id="design-matrix"><a href="#design-matrix" class="headerlink" title="design matrix"></a>design matrix</h2><p>combine t-test with regression (F-test)<br><img src="/2023/08/31/ml/ml2.jpg"><br>batch effect correct<br><img src="/2023/08/31/ml/ml3.jpg"></p>
<p>odd and odd ratio</p>
<h2 id="3-ways-to-determine-if-an-odds-ratio-is-statistically-significant"><a href="#3-ways-to-determine-if-an-odds-ratio-is-statistically-significant" class="headerlink" title="3 ways to determine if an odds ratio is statistically significant."></a>3 ways to determine if an odds ratio is statistically significant.</h2><p>1)Fisher’s Exact Test<br>2)Chi-Square Test<br>3)The Wald Test</p>
<h2 id="coeffecient-of-logistic-regression"><a href="#coeffecient-of-logistic-regression" class="headerlink" title="coeffecient of logistic regression"></a>coeffecient of logistic regression</h2><p>？why the standard deviation is calculated by these<br>and how to calculate the standard error and z-value(&#x3D;Etimate&#x2F;standard error.by wald test.it’s the number of standard deviations the estimated intercept is away from 0 on a standard normal curve)<br><img src="/2023/08/31/ml/intercept.jpg"><br><img src="/2023/08/31/ml/slope.jpg"></p>
<h2 id="R-square-and-p-value-of-logistic-regression"><a href="#R-square-and-p-value-of-logistic-regression" class="headerlink" title="R square and p-value of logistic regression"></a>R square and p-value of logistic regression</h2><p>log-likelihood based R square (aka McFadden’s Pseudo R square)<br><img src="/2023/08/31/ml/ml4.jpg"></p>
<h2 id="Saturated-Model-and-Deviance"><a href="#Saturated-Model-and-Deviance" class="headerlink" title="Saturated Model and Deviance"></a>Saturated Model and Deviance</h2><p>LL(Saturated model)always&#x3D;0 because the saturated model fit the spot well.<br><img src="/2023/08/31/ml/ml5.jpg"><br><img src="/2023/08/31/ml/ml6.jpg"></p>
<p>AIC: Akaike Information Criterion. Residual adjusted for the number of parameters. can be used to compare one model to another.</p>
<h2 id="logistic-regression-code"><a href="#logistic-regression-code" class="headerlink" title="logistic regression code"></a>logistic regression <a target="_blank" rel="noopener" href="https://github.com/StatQuest/logistic_regression_demo/blob/master/logistic_regression_demo.R">code</a></h2><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"># chi-square value = 2*(LL(Proposed) - LL(Null))</span><br><span class="line"># p-value = 1 - pchisq(chi-square value, df = 2-1)</span><br><span class="line">1 - pchisq(2*(ll.proposed - ll.null), df=1)</span><br><span class="line">1 - pchisq((logistic$null.deviance - logistic$deviance), df=1)</span><br></pre></td></tr></table></figure>
<p>and predict.</p>
<h2 id="Deviance-residuals"><a href="#Deviance-residuals" class="headerlink" title="Deviance residuals"></a>Deviance residuals</h2><p>the square root of the contribution that each data point has to the overall Residual Deviance. used to identify outliers.</p>
<h2 id="PCA"><a href="#PCA" class="headerlink" title="PCA"></a>PCA</h2><p>using SVD(singular value decomposition)<br>singular vector&#x2F;eigenvector<br>loading scores<br>eigenvalue<br><img src="/2023/08/31/ml/pca1.jpg"><br><img src="/2023/08/31/ml/pca2.jpg"><br><img src="/2023/08/31/ml/pca3.jpg"><br>Tips:scaling,centering,and verify the number of principal components.<br><a target="_blank" rel="noopener" href="https://github.com/StatQuest/pca_demo/blob/master/pca_demo.R">code in R</a><br>note: prcomp require samples on rows. square pca$sdev can be used for %(eigenvector). rotation &#x3D; loading scores<br><a target="_blank" rel="noopener" href="https://github.com/StatQuest/pca_demo/blob/master/pca_demo.py">code in python</a><br><code>StandardScaler().fit_transform(data.T)</code></p>
<h2 id="LDA"><a href="#LDA" class="headerlink" title="LDA"></a>LDA</h2><p>maximizing the seperatibility between 2 groups while PCA by focusing on the genes with the most variation.<br>2 criteria: Maximize the distance between means; minimize the variation (scatter;square s)within each category.<br><img src="/2023/08/31/ml/lda.jpg"></p>
<h2 id="MDS-and-PCoA"><a href="#MDS-and-PCoA" class="headerlink" title="MDS and PCoA"></a>MDS and PCoA</h2><p><img src="/2023/08/31/ml/mds.jpg"></p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">dist()</span><br><span class="line">cmdscale()</span><br></pre></td></tr></table></figure>
<p><a target="_blank" rel="noopener" href="https://github.com/StatQuest/mds_and_pcoa_demo/blob/master/mds_and_pcoa_demo.R">code</a></p>
<h2 id="t-SNE"><a href="#t-SNE" class="headerlink" title="t-SNE"></a>t-SNE</h2><h2 id="Hierarchical-Clustering"><a href="#Hierarchical-Clustering" class="headerlink" title="Hierarchical Clustering"></a>Hierarchical Clustering</h2><p>similarity(distance) to form clusters;<br>decide how to compare sub-clusters:<br>centroid<br>single-linkage<br>complete-linkage;<br>height of the branches in the ‘dendrogram’ shows what is most similar.</p>
<h2 id="k-means-clustering"><a href="#k-means-clustering" class="headerlink" title="k-means clustering"></a>k-means clustering</h2><p>elbow plot to find ‘K’</p>
<h2 id="DBSCAN"><a href="#DBSCAN" class="headerlink" title="DBSCAN"></a>DBSCAN</h2><p>radius of circle for a initial cluster<br>core point</p>
<h2 id="K-nearest-neighbors"><a href="#K-nearest-neighbors" class="headerlink" title="K-nearest neighbors"></a>K-nearest neighbors</h2><h2 id="Naive-Bayes"><a href="#Naive-Bayes" class="headerlink" title="Naive Bayes"></a>Naive Bayes</h2><ul>
<li>Multinomial Naive Bayes Classifier<br>  probability for discrete called likelihood<br>  set alpha to avoid 0<br>  high bia and low variance in ML</li>
<li>Gaussian Naive Bayes Classification<br>  use log() to avoid ‘uderflow’</li>
</ul>
<h2 id="Decision-and-Classification-tree"><a href="#Decision-and-Classification-tree" class="headerlink" title="Decision and Classification tree"></a>Decision and Classification tree</h2><p>test leaves impure: gini impurity, entropy and information gain.<br>how to calculate gini impurity:<br><img src="/2023/08/31/ml/impurity1.jpg"><br><img src="/2023/08/31/ml/impurity2.jpg"><br>to calculate for numeric data, using average age for all adjacent people.<br>put lowest impurity leaves as node.<br>overfit if sample too small -&gt; pruning<br>                           or -&gt; put limits on how trees grow(i.e. require x samples per leaf, x is determined by cross varidation)<br>feature selection: cut off the feature not reduce the impurity to avoid overfit.</p>
<h2 id="Regression-trees"><a href="#Regression-trees" class="headerlink" title="Regression trees"></a>Regression trees</h2><p>determine thresholds by calculating the sum of squared residual, pick up the smallest one.</p>
<h3 id="prune-regression-tree"><a href="#prune-regression-tree" class="headerlink" title="prune regression tree:"></a>prune regression tree:</h3><p>cost complexity pruning(aka weakest link pruning)<br>– tree score &#x3D; SSR + alpha * T<br>(T represent the total number of leaves; alpha T &#x3D; tree complexity panelty)<br>– set which <strong>alpha</strong> gave us the lowest sum of squared ressiduals with the testing data；<strong>tree</strong> built by full data(train + test) </p>
<p>? what is <strong>new</strong> training data and testing data</p>
<h2 id="Encoding"><a href="#Encoding" class="headerlink" title="Encoding"></a>Encoding</h2><p>one-hot encoding: each discrete option as a colname.<br>label encoding: as 1,2,3 but not good for meaning.<br>target encoding: mean for each discrete option.<br>less samples less confidence<br>so <strong>Baysian Mean Encoding</strong><br>data leakage -&gt; overfit<br>so <strong>k-fold Target encoding</strong><br>leave-one-out-tatget encoding<br><img src="/2023/08/31/ml/encoding.jpg"><br>setting m &#x3D;2 means we need at least 3 rows of data before the option mean, the mean we calculated for blue, becomes more important than the overall mean. ? Does this mean 2 &lt; 3</p>
<p>Classification Trees in Python from Start to Finish. <a target="_blank" rel="noopener" href="https://github.com/Victor-cb/Classification-Trees-in-Python-From-Start-To-Finish/blob/master/classification_trees.ipynb">code &amp; tutorial</a></p>
<h2 id="random-forest"><a href="#random-forest" class="headerlink" title="random forest"></a>random forest</h2><p>bootstrapping the data plus using the aggregate to make a decision is called <strong>bagging</strong>.<br><strong>out-of-bag</strong> dataset used to measure accuracy.<br><strong>out-of-bag error</strong>:the proportion of incorrect classified.</p>
<ul>
<li>fill-in missing values<br>proximity matrix: the time of <samples of column and row have same result> &#x2F; trees number<br>– orifinal dataset :for dicrete item: the weighted frequency for ‘yes’ is the frequency of ‘yes’ * the weight for ‘yes’<br>the weight for ‘yes’&#x3D; proximity of ‘yes’&#x2F;all proximities<br>iterative<br>distance&#x3D; 1-proximity value -&gt;heatmap&#x2F;MDS<br>– new sample: for dicrete item :guess each result and option, then iterative calculate, choose the most correct option, finally fill-in the data and classify the sample.</li>
</ul>
<p><a target="_blank" rel="noopener" href="https://github.com/StatQuest/random_forest_demo/blob/master/random_forest_demo.R">code</a><br>##mtry</p>
<h2 id="loss-function"><a href="#loss-function" class="headerlink" title="loss function"></a>loss function</h2><p>fix slope to find optimal intercept.<br>the chain rule (拆分求导)<br>sum of the squaired residuals is a type of loss function.</p>
<h2 id="gradient-descent"><a href="#gradient-descent" class="headerlink" title="gradient descent"></a>gradient descent</h2><p>for LR: step size &#x3D; slope * learning rates (minimum &#x3D; 0.001)<br>note: the results is sensitive to <strong>learning rates</strong>,the way we change lr is called <strong>schedule</strong>.</p>
<ul>
<li>new intercept &#x3D; old intercept - step size<br>(also can be used to find both optimal slope and intercept)<br><strong>stochastic gradient descent</strong> which select subset of data rather than full dataset.<br>using 1 or mini-batch of samples for each step. easily update the parameters when add new data.<br>can be used to logistic regression and t-sne.</li>
</ul>
<h2 id="AdaBoost"><a href="#AdaBoost" class="headerlink" title="AdaBoost"></a>AdaBoost</h2><p>(<strong>stumps</strong> is a node with 2 leaves.)<br><strong>Amount of Say</strong><br><img src="/2023/08/31/ml/adaboost1.jpg"><br>Weighted Gini Function -&gt; sample weights<br><img src="/2023/08/31/ml/adaboost2.jpg"></p>
<h2 id="Gradient-Boost"><a href="#Gradient-Boost" class="headerlink" title="Gradient Boost"></a>Gradient Boost</h2><ul>
<li><strong>Gradient Boost</strong> for <strong>Regression</strong> is different from doing <strong>Linear Regression</strong>.<br><img src="/2023/08/31/ml/gradientboost1.jpg"><br><img src="/2023/08/31/ml/gradientboost2.jpg"><br>M often &#x3D; 100</li>
<li>for classification<br>log(odds)<br>set predicted xx<br>logistic function -&gt; probability<br>limiting the number of leaves between 8 and 32.<br><strong>Pseudo residuals</strong> means change residual by multiply to make calculate more simple.<br>the larger log(likelihood),the better prediction.<br>log(likelihood) multiply -1 as <strong>loss function</strong>.(argmin)<br>using 2nd order <strong>Taylor Polynomial</strong> in (A)<br><img src="/2023/08/31/ml/gradientboost3.jpg"><br>(A)calculate new residuels<br>(B)create new regression tree<br>(C)calculate output value<br>(D)make new prediction (log(odds))</li>
</ul>
<h2 id="XGBoost-eXtreme-Gradient-Boost"><a href="#XGBoost-eXtreme-Gradient-Boost" class="headerlink" title="XGBoost(eXtreme Gradient Boost)"></a>XGBoost(eXtreme Gradient Boost)</h2><ul>
<li>regression</li>
<li>classification<br>similarity score<br>lambda: a <strong>Regularization Parameter</strong>, which reduce the prediction’s sensitivity to individual observations. prevented overfit. smaller output values for leaves.<br>gain-gamma -&gt; prune or not<br>eta (learning rate)<br>cover min_child_weight -&gt; prune<br>?predicted drug effectiveness &#x3D; 0.5<br><img src="/2023/08/31/ml/xgboost1.jpg"><br>pick optimal outputvalue to make L equation min<br>regularization penalty by increasing lambda, the optimal outputvalue gets closer to 0.<br>gradients (g); hessians (h)<br><img src="/2023/08/31/ml/xgboost2.jpg"><br>first 3 part is to predict, next is to <strong>Optimization</strong> for large dataset.</li>
<li>greedy algorithm<br>quantile (default 33)</li>
<li>sketch algorithms<br>weighted quantile sketch<br>weight &#x3D; previous probability * (1 - previous probability)</li>
<li>Sparsity-Aware Split Fingding<br>for missing data</li>
</ul>
<h2 id="Cosine-Similarity"><a href="#Cosine-Similarity" class="headerlink" title="Cosine Similarity"></a>Cosine Similarity</h2><h2 id="Support-Vector-Machines"><a href="#Support-Vector-Machines" class="headerlink" title="Support Vector Machines"></a>Support Vector Machines</h2><p>Maximal Margin Classifiers<br>support vector classifier aka soft margin classifiers<br>above 2 can not handle many cluster of data, so we use support vector machines to find a relatively <strong>high</strong> dimensional support vector classifier.<br>hyperplane</p>
<p>Function: </p>
<ol>
<li>polynomial kernel</li>
</ol>
<ul>
<li>(a*b + r)^d<br>r,d varified by cross validation.</li>
</ul>
<ol start="2">
<li>radial kernel aka radial basis function(RBF)kernel: in infinite dimension. behaves like <strong>weighted Nearest Neighbor</strong> model.</li>
</ol>
<ul>
<li>e ^ (-gamma(a-b)^2)<br>gamma determined by cross validation.<br><strong>Taylor Series Expansion</strong><br><img src="/2023/08/31/ml/taylor.jpg"><br><img src="/2023/08/31/ml/radialkernel.jpg"><br><strong>Dot product</strong><br>when we plug numbers into radial kernel, the value we get is the relationship between the 2 points in infinite-dimensions.</li>
</ul>
<h2 id="Neural-Networks"><a href="#Neural-Networks" class="headerlink" title="Neural Networks"></a>Neural Networks</h2><p>Backpropagation<br>Hidden Layers<br>Activation Function:</p>
<ul>
<li>sigmoid  e^x&#x2F;(e^x + 1)</li>
<li>ReLU (Rectified Linear Unit) max(0,x)</li>
<li>softplus log(1 + e^x)<br>initialize <strong>weights</strong> using a standard normal distribution.(one of many ways)<br>initialize <strong>bias</strong>  to 0<br>multi-output: <strong>ArgMax</strong>(0or1) can not be used for backpropagation.<br>or SortMax(e^x&#x2F;e^x+e^y) used for gradient descent because of the derivative not being 0.</li>
<li>calculate deviation of sortmax -&gt; Quotient Rule<br><img src="/2023/08/31/ml/quotient-rule-formula.png"><br><img src="/2023/08/31/ml/softmax.jpg"></li>
</ul>
<p><strong>Cross Entropy</strong></p>

        </div>

    </div>

    

    

    

    

    

    
<nav class="article-nav">
  
    <a href="/2122/11/17/Hi/" id="article-nav-newer" class="article-nav-link-wrap">
      <div class="article-nav-caption">下一篇</div>
      <div class="article-nav-title">
        
          (no title)
        
      </div>
    </a>
  
  
    <a href="/2023/08/09/leafcutter/" id="article-nav-older" class="article-nav-link-wrap">
      <div class="article-nav-caption">上一篇</div>
      <div class="article-nav-title">leafcutter 使用说明</div>
    </a>
  
</nav>


    <section class="share">
        <div class="share-title">分享</div>
        <a class="share-item" target="_blank"
            href="https://twitter.com/share?text=The Summary of Machine learning (StatQuest) - leilei's blog&url=http%3A%2F%2Fexample.com%2F2023%2F08%2F31%2Fml%2F">
            <ion-icon name="logo-twitter"></ion-icon>
        </a>
        <a class="share-item" target="_blank"
            href="https://www.facebook.com/sharer.php?title=The Summary of Machine learning (StatQuest) - leilei's blog&u=http%3A%2F%2Fexample.com%2F2023%2F08%2F31%2Fml%2F">
            <ion-icon name="logo-facebook"></ion-icon>
        </a>
        <!-- <a class="share-item" target="_blank"
            href="https://service.weibo.com/share/share.php?title=The Summary of Machine learning (StatQuest) - leilei's blog&url=http://example.com/2023/08/31/ml/&pic=">
            <div class="n-icon n-icon-weibo"></div>
        </a> -->
    </section>

</article>
















</div>
                </section>
            </section>

            
            <aside class="sidebar ">
                


<div class="widget" id="widget">
    
      
  <div class="widget-wrap widget-cate">
    <div class="widget-title"><span>Categories</span></div>
    <div class="widget-inner">
      <ul class="category-list"><li class="category-list-item"><a class="category-list-link" href="/categories/My-Article/">My Article</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/My-Outline/">My Outline</a></li></ul>
    </div>
  </div>


    
      
  <div class="widget-wrap widget-tags">
    <div class="widget-title"><span>Tags</span></div>
    <div class="widget-inner">
      <ul class="tag-list" itemprop="keywords"><li class="tag-list-item"><a class="tag-list-link" href="/tags/My-Article/" rel="tag">My Article</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/My-Outline/" rel="tag">My Outline</a></li></ul>
    </div>
  </div>


    
      
  <div class="widget-wrap widget-recent-posts">
    <div class="widget-title"><span>Recent Posts</span></div>
    <div class="widget-inner">
      <ul>
        
          <li>
            <a href="/2122/11/17/Hi/">(no title)</a>
          </li>
        
          <li>
            <a href="/2023/08/31/ml/">The Summary of Machine learning (StatQuest)</a>
          </li>
        
          <li>
            <a href="/2023/08/09/leafcutter/">leafcutter 使用说明</a>
          </li>
        
          <li>
            <a href="/2023/05/24/LDAK/">LDAK</a>
          </li>
        
          <li>
            <a href="/2023/04/12/DataScience/">DataScience</a>
          </li>
        
      </ul>
    </div>
  </div>

    
      
  <div class="widget-wrap widget-archive">
    <div class="widget-title"><span>Archive</span></div>
    <div class="widget-inner">
      <ul class="archive-list"><li class="archive-list-item"><a class="archive-list-link" href="/archives/2122/">2122</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/">2023</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2022/">2022</a></li></ul>
    </div>
  </div>


    
</div>

<div id="backtop"><i class="icon icon-arrow-up"></i></div>
            </aside>
            
        </div>
    </div>

    <footer class="footer">
    <div class="footer-wave">
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 1440 320"><path fill="#3c4859" fill-opacity="1" d="M0,160L60,181.3C120,203,240,245,360,240C480,235,600,181,720,186.7C840,192,960,256,1080,261.3C1200,267,1320,213,1380,186.7L1440,160L1440,320L1380,320C1320,320,1200,320,1080,320C960,320,840,320,720,320C600,320,480,320,360,320C240,320,120,320,60,320L0,320Z"></path></svg>
    </div>

    <!-- Please do not remove this -->
    <!-- 开源不易，请勿删除 -->
    <div class="footer-wrap">
        <div class="footer-inner"> 
            leilei&#39;s blog &copy; 2023<br>
            Powered By Hexo · Theme By <a href="https://linhong.me/" target="_blank">Aomori</a> · <a href="https://github.com/lh1me/hexo-theme-aomori" target="_blank">Github</a>
        </div>
    </div>

</footer>

<script type="module" src="https://unpkg.com/ionicons@6.0.2/dist/ionicons/ionicons.esm.js"></script>






<script src="/dist/build.js?1654266144177.js"></script>


<script src="/dist/custom.js?1654266144177.js"></script>













</body>

</html>